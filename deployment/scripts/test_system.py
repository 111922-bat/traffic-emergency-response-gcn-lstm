#!/usr/bin/env python3
"""
éƒ¨ç½²ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯•è„šæœ¬
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path

# æ·»åŠ éƒ¨ç½²æ¨¡å—è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from compression.model_pruning import UnstructuredPruner
from compression.knowledge_distillation import KnowledgeDistiller
from quantization.model_quantization import QuantizationOptimizer
from caching.model_cache import ModelCache
from optimization.deployment_architecture import SystemAnalyzer
from optimization.memory_optimization import MemoryOptimizer


class SimpleModel(nn.Module):
    """ç®€å•çš„æµ‹è¯•æ¨¡å‹"""
    def __init__(self, input_size=10, hidden_size=50, output_size=1):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc3 = nn.Linear(hidden_size // 2, output_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


def create_test_data(num_samples=1000, input_size=10):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    X = torch.randn(num_samples, input_size)
    y = torch.randn(num_samples, 1)
    return X, y


def test_model_pruning():
    """æµ‹è¯•æ¨¡å‹å‰ªæ"""
    print("=== æµ‹è¯•æ¨¡å‹å‰ªæ ===")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = SimpleModel()
    test_data, test_labels = create_test_data(100)
    
    # æµ‹è¯•éç»“æ„åŒ–å‰ªæ
    pruner = UnstructuredPruner(model, sparsity_ratio=0.3)
    pruned_model = pruner.prune_model()
    
    # éªŒè¯æ¨¡å‹ç»“æ„
    original_params = sum(p.numel() for p in model.parameters())
    pruned_params = sum(p.numel() for p in pruned_model.parameters())
    
    print(f"åŸå§‹æ¨¡å‹å‚æ•°: {original_params}")
    print(f"å‰ªæåå‚æ•°: {pruned_params}")
    print(f"å‰ªææ¯”ä¾‹: {(original_params - pruned_params) / original_params * 100:.2f}%")
    print("âœ… æ¨¡å‹å‰ªææµ‹è¯•é€šè¿‡")
    return pruned_model


def test_knowledge_distillation():
    """æµ‹è¯•çŸ¥è¯†è’¸é¦"""
    print("\n=== æµ‹è¯•çŸ¥è¯†è’¸é¦ ===")
    
    # åˆ›å»ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹
    teacher_model = SimpleModel(hidden_size=100)
    student_model = SimpleModel(hidden_size=50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    train_data, train_labels = create_test_data(200)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
    
    # æµ‹è¯•è’¸é¦å™¨
    distiller = KnowledgeDistiller(
        teacher_model, student_model,
        temperature=4.0, alpha=0.7, beta=0.3
    )
    
    print("è’¸é¦å™¨åˆ›å»ºæˆåŠŸ")
    print("âœ… çŸ¥è¯†è’¸é¦æµ‹è¯•é€šè¿‡")
    return student_model


def test_model_quantization():
    """æµ‹è¯•æ¨¡å‹é‡åŒ–"""
    print("\n=== æµ‹è¯•æ¨¡å‹é‡åŒ– ===")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = SimpleModel()
    test_data, test_labels = create_test_data(100)
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(test_data, test_labels)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)
    
    # æµ‹è¯•é‡åŒ–ä¼˜åŒ–å™¨
    optimizer = QuantizationOptimizer(model)
    
    print("é‡åŒ–ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
    print("âœ… æ¨¡å‹é‡åŒ–æµ‹è¯•é€šè¿‡")
    return model


def test_model_cache():
    """æµ‹è¯•æ¨¡å‹ç¼“å­˜"""
    print("\n=== æµ‹è¯•æ¨¡å‹ç¼“å­˜ ===")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = SimpleModel()
    
    # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
    cache_manager = ModelCache(cache_dir='./test_cache', max_cache_size_gb=0.1)
    
    # æµ‹è¯•ç¼“å­˜
    cache_key = cache_manager.cache_model(model, "test_model", {"input_size": 10})
    print(f"ç¼“å­˜é”®: {cache_key}")
    
    # æµ‹è¯•åŠ è½½
    new_model = SimpleModel()
    success = cache_manager.load_model(new_model, "test_model", {"input_size": 10})
    print(f"åŠ è½½æˆåŠŸ: {success}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = cache_manager.get_cache_stats()
    print(f"ç¼“å­˜ç»Ÿè®¡: {stats}")
    
    # æ¸…ç†
    cache_manager.clear_cache()
    
    print("âœ… æ¨¡å‹ç¼“å­˜æµ‹è¯•é€šè¿‡")


def test_system_analysis():
    """æµ‹è¯•ç³»ç»Ÿåˆ†æ"""
    print("\n=== æµ‹è¯•ç³»ç»Ÿåˆ†æ ===")
    
    # è·å–ç³»ç»Ÿè§„æ ¼
    specs = SystemAnalyzer.get_system_specs()
    print(f"CPUæ ¸å¿ƒæ•°: {specs.cpu_cores}")
    print(f"å†…å­˜å¤§å°: {specs.memory_gb:.1f}GB")
    print(f"GPUæ•°é‡: {specs.gpu_count}")
    print(f"æ“ä½œç³»ç»Ÿ: {specs.os_type}")
    
    # åˆ†æéƒ¨ç½²é€‚åˆæ€§
    analyzer = SystemAnalyzer()
    analysis = analyzer.analyze_deployment_suitability(specs)
    
    print(f"æ¨èéƒ¨ç½²ç±»å‹: {analysis['recommended_deployment']}")
    print(f"ä¼˜åŒ–å»ºè®®: {analysis['optimization_suggestions']}")
    
    print("âœ… ç³»ç»Ÿåˆ†ææµ‹è¯•é€šè¿‡")


def test_memory_optimization():
    """æµ‹è¯•å†…å­˜ä¼˜åŒ–"""
    print("\n=== æµ‹è¯•å†…å­˜ä¼˜åŒ– ===")
    
    # åˆ›å»ºå†…å­˜ä¼˜åŒ–å™¨
    optimizer = MemoryOptimizer(memory_limit_gb=1.0)
    
    # ç›‘æ§å†…å­˜
    stats = optimizer.monitor_memory()
    print(f"å½“å‰å†…å­˜ä½¿ç”¨: {stats.used_memory_mb:.2f}MB / {stats.total_memory_mb:.2f}MB")
    
    # æ™ºèƒ½åƒåœ¾å›æ”¶
    optimizer.smart_gc(force=True)
    
    # é¢„æµ‹å†…å­˜å‹åŠ›
    prediction = optimizer.predict_memory_pressure()
    print(f"å†…å­˜å‹åŠ›é¢„æµ‹: {prediction}")
    
    # è·å–ä¼˜åŒ–å»ºè®®
    recommendations = optimizer.get_optimization_recommendations()
    print(f"ä¼˜åŒ–å»ºè®®: {recommendations}")
    
    print("âœ… å†…å­˜ä¼˜åŒ–æµ‹è¯•é€šè¿‡")


def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´ä¼˜åŒ–æµç¨‹"""
    print("\n=== æµ‹è¯•å®Œæ•´ä¼˜åŒ–æµç¨‹ ===")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        model = SimpleModel()
        
        # ä¿å­˜æµ‹è¯•æ¨¡å‹
        os.makedirs('./test_models', exist_ok=True)
        torch.save(model, './test_models/test_model.pth')
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data, test_labels = create_test_data(100)
        
        # åˆ›å»ºè™šæ‹Ÿæ•°æ®åŠ è½½å™¨
        train_dataset = torch.utils.data.TensorDataset(test_data, test_labels)
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)
        
        # æµ‹è¯•å„ä¸ªç»„ä»¶
        print("1. æµ‹è¯•æ¨¡å‹å‰ªæ...")
        pruned_model = test_model_pruning()
        
        print("2. æµ‹è¯•çŸ¥è¯†è’¸é¦...")
        student_model = test_knowledge_distillation()
        
        print("3. æµ‹è¯•æ¨¡å‹é‡åŒ–...")
        quantized_model = test_model_quantization()
        
        print("4. æµ‹è¯•æ¨¡å‹ç¼“å­˜...")
        test_model_cache()
        
        print("5. æµ‹è¯•ç³»ç»Ÿåˆ†æ...")
        test_system_analysis()
        
        print("6. æµ‹è¯•å†…å­˜ä¼˜åŒ–...")
        test_memory_optimization()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼éƒ¨ç½²ä¼˜åŒ–ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        import shutil
        if os.path.exists('./test_cache'):
            shutil.rmtree('./test_cache')
        if os.path.exists('./test_models'):
            shutil.rmtree('./test_models')
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹éƒ¨ç½²ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import torch
        import numpy as np
        import psutil
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return
    
    # è¿è¡Œæµ‹è¯•
    success = test_full_pipeline()
    
    if success:
        print("\n" + "=" * 50)
        print("ğŸŠ éƒ¨ç½²ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. é…ç½®ä¼˜åŒ–å‚æ•°: configs/optimization_config.yaml")
        print("2. è¿è¡Œæ¨¡å‹ä¼˜åŒ–: python scripts/optimize_model.py --full_pipeline")
        print("3. éƒ¨ç½²æœåŠ¡: ./scripts/deploy.sh --deploy-type local")
        print("4. æµ‹è¯•API: curl http://localhost:8080/health")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    return success


if __name__ == "__main__":
    main()