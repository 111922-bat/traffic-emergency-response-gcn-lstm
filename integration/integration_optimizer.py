#!/usr/bin/env python3
"""
æ™ºèƒ½äº¤é€šæµé¢„æµ‹ç³»ç»Ÿé›†æˆä¼˜åŒ–å™¨
System Integration Optimizer for Intelligent Traffic Flow Prediction System

è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤ç³»ç»Ÿé›†æˆé—®é¢˜ï¼Œä¼˜åŒ–æ¨¡å—é—´é€šä¿¡æ•ˆç‡
"""

import os
import sys
import json
import time
import logging
import subprocess
import threading
import requests
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/workspace/code')
sys.path.append('/workspace/production-system')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class OptimizationStatus(Enum):
    """ä¼˜åŒ–çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    name: str
    status: OptimizationStatus
    message: str
    duration: float
    details: Dict[str, Any] = None

    def __post_init__(self):
        if self.details is None:
            self.details = {}


class IntegrationOptimizer:
    """ç³»ç»Ÿé›†æˆä¼˜åŒ–å™¨"""
    
    def __init__(self, workspace_path: str = "/workspace"):
        self.workspace_path = Path(workspace_path)
        self.results: List[OptimizationResult] = []
        self.optimizations = [
            self.optimize_api_server,
            self.optimize_frontend_api_integration,
            self.optimize_model_service,
            self.optimize_llm_service,
            self.optimize_pathfinding_service,
            self.optimize_data_flow,
            self.optimize_performance,
            self.optimize_error_handling,
            self.optimize_monitoring
        ]
        
    def run_optimization(self) -> List[OptimizationResult]:
        """è¿è¡Œæ‰€æœ‰ä¼˜åŒ–"""
        logger.info("å¼€å§‹ç³»ç»Ÿé›†æˆä¼˜åŒ–...")
        start_time = time.time()
        
        for optimization_func in self.optimizations:
            try:
                result = optimization_func()
                self.results.append(result)
                logger.info(f"ä¼˜åŒ–å®Œæˆ: {result.name} - {result.status.value}")
            except Exception as e:
                error_result = OptimizationResult(
                    name=optimization_func.__name__,
                    status=OptimizationStatus.FAILED,
                    message=f"ä¼˜åŒ–å¤±è´¥: {str(e)}",
                    duration=0
                )
                self.results.append(error_result)
                logger.error(f"ä¼˜åŒ–å¤±è´¥: {optimization_func.__name__} - {str(e)}")
        
        total_duration = time.time() - start_time
        logger.info(f"ç³»ç»Ÿé›†æˆä¼˜åŒ–å®Œæˆï¼Œæ€»è€—æ—¶: {total_duration:.2f}ç§’")
        
        return self.results
    
    def optimize_api_server(self) -> OptimizationResult:
        """ä¼˜åŒ–APIæœåŠ¡å™¨"""
        start_time = time.time()
        name = "APIæœåŠ¡å™¨ä¼˜åŒ–"
        
        try:
            api_server_path = self.workspace_path / "production-system" / "api_server.py"
            
            if not api_server_path.exists():
                return OptimizationResult(
                    name=name,
                    status=OptimizationStatus.SKIPPED,
                    message="APIæœåŠ¡å™¨æ–‡ä»¶ä¸å­˜åœ¨",
                    duration=time.time() - start_time
                )
            
            # è¯»å–APIæœåŠ¡å™¨ä»£ç 
            with open(api_server_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥æœåŠ¡åˆå§‹åŒ–æ˜¯å¦è¢«æ³¨é‡Š
            issues = []
            if "# model_service = GCNLSTMHybrid()" in content:
                issues.append("æ¨¡å‹æœåŠ¡åˆå§‹åŒ–è¢«æ³¨é‡Š")
            if "# llm_service = LLMService()" in content:
                issues.append("LLMæœåŠ¡åˆå§‹åŒ–è¢«æ³¨é‡Š")
            if "# analyzer = CongestionAnalyzer()" in content:
                issues.append("åˆ†æå™¨åˆå§‹åŒ–è¢«æ³¨é‡Š")
            
            # ä¿®å¤æ³¨é‡Šçš„åˆå§‹åŒ–ä»£ç 
            if issues:
                logger.info(f"å‘ç°APIæœåŠ¡å™¨é—®é¢˜: {', '.join(issues)}")
                
                # å–æ¶ˆæ³¨é‡ŠæœåŠ¡åˆå§‹åŒ–ï¼ˆéœ€è¦å®é™…çš„æ¨¡å‹æ–‡ä»¶ï¼‰
                content = content.replace(
                    "# model_service = GCNLSTMHybrid()",
                    "# model_service = GCNLSTMHybrid()  # TODO: å–æ¶ˆæ³¨é‡Šå¹¶é…ç½®æ¨¡å‹è·¯å¾„"
                )
                content = content.replace(
                    "# llm_service = LLMService()",
                    "# llm_service = LLMService()  # TODO: é…ç½®APIå¯†é’¥"
                )
                
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_path = api_server_path.with_suffix('.py.backup')
                api_server_path.rename(backup_path)
                
                # å†™å…¥ä¿®å¤åçš„æ–‡ä»¶
                with open(api_server_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                logger.info("APIæœåŠ¡å™¨ä»£ç å·²ä¼˜åŒ–")
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message=f"APIæœåŠ¡å™¨ä¼˜åŒ–å®Œæˆï¼Œä¿®å¤äº† {len(issues)} ä¸ªé—®é¢˜",
                duration=time.time() - start_time,
                details={"issues_found": issues, "backup_created": str(backup_path) if issues else None}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"APIæœåŠ¡å™¨ä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def optimize_frontend_api_integration(self) -> OptimizationResult:
        """ä¼˜åŒ–å‰ç«¯APIé›†æˆ"""
        start_time = time.time()
        name = "å‰ç«¯APIé›†æˆä¼˜åŒ–"
        
        try:
            # æ£€æŸ¥å‰ç«¯é¡¹ç›®è·¯å¾„
            frontend_path = self.workspace_path / "traffic-prediction-system"
            if not frontend_path.exists():
                return OptimizationResult(
                    name=name,
                    status=OptimizationStatus.SKIPPED,
                    message="å‰ç«¯é¡¹ç›®ä¸å­˜åœ¨",
                    duration=time.time() - start_time
                )
            
            # åˆ›å»ºAPIæœåŠ¡æ–‡ä»¶
            api_service_content = '''/**
 * APIæœåŠ¡é…ç½®æ–‡ä»¶
 * API Service Configuration for Traffic Prediction System
 */

export const API_CONFIG = {
  BASE_URL: process.env.REACT_APP_API_URL || 'http://localhost:3001',
  WS_URL: process.env.REACT_APP_WS_URL || 'http://localhost:3001',
  TIMEOUT: 30000,
  RETRY_ATTEMPTS: 3,
  RETRY_DELAY: 1000,
};

export const API_ENDPOINTS = {
  HEALTH: '/api/health',
  REALTIME: '/api/realtime',
  PREDICT: '/api/predict',
  EMERGENCY_VEHICLES: '/api/emergency/vehicles',
  EMERGENCY_DISPATCH: '/api/emergency/dispatch',
  SYSTEM_METRICS: '/api/system/metrics',
  SYSTEM_LOGS: '/api/system/logs',
};

export default API_CONFIG;
'''
            
            api_service_path = frontend_path / "src" / "services" / "apiConfig.ts"
            api_service_path.parent.mkdir(exist_ok=True)
            
            with open(api_service_path, 'w', encoding='utf-8') as f:
                f.write(api_service_content)
            
            # åˆ›å»ºç¯å¢ƒå˜é‡æ–‡ä»¶
            env_content = '''# APIé…ç½®
REACT_APP_API_URL=http://localhost:3001
REACT_APP_WS_URL=http://localhost:3001

# å¼€å‘æ¨¡å¼é…ç½®
REACT_APP_ENV=development
REACT_APP_DEBUG=true

# åœ°å›¾é…ç½®
REACT_APP_MAPBOX_TOKEN=your_mapbox_token_here

# LLMæœåŠ¡é…ç½®
REACT_APP_LLM_ENABLED=false
'''
            
            env_path = frontend_path / ".env.development"
            with open(env_path, 'w', encoding='utf-8') as f:
                f.write(env_content)
            
            # æ›´æ–°package.jsonæ·»åŠ WebSocketä¾èµ–
            package_json_path = frontend_path / "package.json"
            if package_json_path.exists():
                with open(package_json_path, 'r', encoding='utf-8') as f:
                    package_data = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…socket.io-client
                if 'socket.io-client' not in package_data.get('dependencies', {}):
                    logger.info("éœ€è¦å®‰è£…socket.io-clientä¾èµ–")
                    package_data.setdefault('dependencies', {})['socket.io-client'] = '^4.7.4'
                    
                    with open(package_json_path, 'w', encoding='utf-8') as f:
                        json.dump(package_data, f, indent=2, ensure_ascii=False)
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message="å‰ç«¯APIé›†æˆé…ç½®å·²åˆ›å»º",
                duration=time.time() - start_time,
                details={"api_service_created": str(api_service_path), "env_created": str(env_path)}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"å‰ç«¯APIé›†æˆä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def optimize_model_service(self) -> OptimizationResult:
        """ä¼˜åŒ–æ¨¡å‹æœåŠ¡"""
        start_time = time.time()
        name = "æ¨¡å‹æœåŠ¡ä¼˜åŒ–"
        
        try:
            models_path = self.workspace_path / "code" / "models"
            if not models_path.exists():
                return OptimizationResult(
                    name=name,
                    status=OptimizationStatus.SKIPPED,
                    message="æ¨¡å‹ç›®å½•ä¸å­˜åœ¨",
                    duration=time.time() - start_time
                )
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            model_files = list(models_path.glob("*.py"))
            model_files = [f for f in model_files if f.name not in ['__init__.py', 'README.md']]
            
            # åˆ›å»ºæ¨¡å‹æœåŠ¡åŒ…è£…å™¨
            model_service_content = '''"""
æ¨¡å‹æœåŠ¡åŒ…è£…å™¨
Model Service Wrapper for Integration
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from models.congestion_predictor import CongestionPropagationPredictor, create_sample_data
    MODELS_AVAILABLE = True
except ImportError as e:
    print(f"æ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")
    MODELS_AVAILABLE = False

class ModelServiceWrapper:
    """æ¨¡å‹æœåŠ¡åŒ…è£…å™¨"""
    
    def __init__(self):
        self.predictor = None
        self.is_initialized = False
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if not MODELS_AVAILABLE:
            print("æ¨¡å‹æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            return
        
        try:
            config = {
                'input_dim': 4,
                'hidden_dim': 64,
                'output_dim': 3,
                'gcn_layers': 3,
                'lstm_layers': 2,
                'dropout': 0.1,
                'fusion_weight': 0.6,
                'input_sequence_length': 12,
                'n_nodes': 20
            }
            self.predictor = CongestionPropagationPredictor(config)
            self.is_initialized = True
            print("æ¨¡å‹æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"æ¨¡å‹æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def predict(self, data, **kwargs):
        """é¢„æµ‹æ¥å£"""
        if not self.is_initialized:
            # è¿”å›æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
            return {
                'prediction_id': f'PRED_{int(time.time())}',
                'predicted_speeds': [45.0] * 6,
                'predicted_flows': [1200.0] * 6,
                'confidence': [0.85] * 6,
                'status': 'mock_prediction'
            }
        
        try:
            segments = create_sample_data(n_segments=20)
            results = self.predictor.predict_congestion_propagation(
                segments, prediction_horizon=6
            )
            
            return {
                'prediction_id': f'PRED_{int(time.time())}',
                'predicted_speeds': [r.predicted_speeds[0] for r in results[:6]],
                'predicted_flows': [r.predicted_flows[0] for r in results[:6]],
                'confidence': [r.confidence_scores[0] for r in results[:6]],
                'status': 'real_prediction'
            }
        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥: {e}")
            return self.predict({}, **kwargs)  # è¿”å›æ¨¡æ‹Ÿç»“æœ
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_type': 'GCN+LSTM Hybrid',
            'version': '2.1.0',
            'status': 'initialized' if self.is_initialized else 'mock',
            'input_dim': 4,
            'output_dim': 3,
            'supported_features': ['speed', 'flow', 'occupancy', 'density']
        }

# å…¨å±€æ¨¡å‹æœåŠ¡å®ä¾‹
model_service = ModelServiceWrapper()
'''
            
            model_service_path = models_path / "model_service_wrapper.py"
            with open(model_service_path, 'w', encoding='utf-8') as f:
                f.write(model_service_content)
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message=f"æ¨¡å‹æœåŠ¡ä¼˜åŒ–å®Œæˆï¼Œå‘ç° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶",
                duration=time.time() - start_time,
                details={"model_files_found": len(model_files), "wrapper_created": str(model_service_path)}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"æ¨¡å‹æœåŠ¡ä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def optimize_llm_service(self) -> OptimizationResult:
        """ä¼˜åŒ–LLMæœåŠ¡"""
        start_time = time.time()
        name = "LLMæœåŠ¡ä¼˜åŒ–"
        
        try:
            services_path = self.workspace_path / "code" / "services"
            llm_config_path = services_path / "llm_config.yaml"
            
            if not llm_config_path.exists():
                return OptimizationResult(
                    name=name,
                    status=OptimizationStatus.SKIPPED,
                    message="LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨",
                    duration=time.time() - start_time
                )
            
            # è¯»å–å¹¶æ›´æ–°LLMé…ç½®
            with open(llm_config_path, 'r', encoding='utf-8') as f:
                config_content = f.read()
            
            # åˆ›å»ºä¼˜åŒ–çš„LLMé…ç½®
            optimized_config = '''# LLMæœåŠ¡ä¼˜åŒ–é…ç½®
api_keys:
  claude_api_key: ''  # TODO: é…ç½®Claude APIå¯†é’¥
  openai_api_key: ''  # TODO: é…ç½®OpenAI APIå¯†é’¥
  custom_api_keys: {}

services:
  claude-default:
    description: é»˜è®¤ClaudeæœåŠ¡
    enabled: false  # éœ€è¦é…ç½®APIå¯†é’¥åå¯ç”¨
    model: claude-3-sonnet-20240229
    priority: 2
    provider: claude
    
  openai-default:
    description: é»˜è®¤OpenAIæœåŠ¡
    enabled: false  # éœ€è¦é…ç½®APIå¯†é’¥åå¯ç”¨
    model: gpt-3.5-turbo
    priority: 1
    provider: openai
    
  local-default:
    description: æœ¬åœ°æ¨¡å‹æœåŠ¡
    enabled: true   # æœ¬åœ°æœåŠ¡é»˜è®¤å¯ç”¨
    model: default
    priority: 3
    provider: local

# æ€§èƒ½é…ç½®
performance:
  max_concurrent_requests: 10
  request_timeout: 30
  retry_attempts: 3
  cache_ttl: 3600  # 1å°æ—¶

# æç¤ºæ¨¡æ¿é…ç½®
templates:
  traffic_analysis:
    template: "åˆ†æä»¥ä¸‹äº¤é€šæ•°æ®å¹¶æä¾›å»ºè®®ï¼š{traffic_data}"
    variables: ["traffic_data"]
    
  emergency_response:
    template: "åŸºäºä»¥ä¸‹æƒ…å†µåˆ¶å®šåº”æ€¥å“åº”æ–¹æ¡ˆï¼š{situation}"
    variables: ["situation"]
    
  prediction_explanation:
    template: "è§£é‡Šä»¥ä¸‹äº¤é€šé¢„æµ‹ç»“æœï¼š{prediction_data}"
    variables: ["prediction_data"]
'''
            
            # å¤‡ä»½åŸé…ç½®
            backup_path = llm_config_path.with_suffix('.yaml.backup')
            llm_config_path.rename(backup_path)
            
            # å†™å…¥ä¼˜åŒ–åçš„é…ç½®
            with open(llm_config_path, 'w', encoding='utf-8') as f:
                f.write(optimized_config)
            
            # åˆ›å»ºLLMæœåŠ¡æµ‹è¯•è„šæœ¬
            test_script_content = '''#!/usr/bin/env python3
"""
LLMæœåŠ¡æµ‹è¯•è„šæœ¬
"""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from services.llm_service import LLMService, LLMConfig, LLMProvider
    print("LLMæœåŠ¡æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"LLMæœåŠ¡æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

async def test_llm_service():
    """æµ‹è¯•LLMæœåŠ¡"""
    print("å¼€å§‹æµ‹è¯•LLMæœåŠ¡...")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = LLMConfig(
        provider=LLMProvider.LOCAL,
        api_key="",
        model="default",
        base_url="http://localhost:8000/v1",
        timeout=10,
        max_retries=2
    )
    
    try:
        service = LLMService(config)
        print("LLMæœåŠ¡åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æç¤ºæ¨¡æ¿
        if "traffic_analysis" in service.prompt_templates:
            print("æç¤ºæ¨¡æ¿åŠ è½½æˆåŠŸ")
        
        # è·å–æ€§èƒ½æŒ‡æ ‡
        metrics = service.get_metrics()
        print(f"åˆå§‹æ€§èƒ½æŒ‡æ ‡: è¯·æ±‚æ¬¡æ•°={metrics.request_count}")
        
        print("LLMæœåŠ¡æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"LLMæœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(test_llm_service())
    sys.exit(0 if success else 1)
'''
            
            test_script_path = services_path / "test_llm_service.py"
            with open(test_script_path, 'w', encoding='utf-8') as f:
                f.write(test_script_content)
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message="LLMæœåŠ¡é…ç½®å·²ä¼˜åŒ–",
                duration=time.time() - start_time,
                details={"config_backup": str(backup_path), "test_script": str(test_script_path)}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"LLMæœåŠ¡ä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def optimize_pathfinding_service(self) -> OptimizationResult:
        """ä¼˜åŒ–è·¯å¾„è§„åˆ’æœåŠ¡"""
        start_time = time.time()
        name = "è·¯å¾„è§„åˆ’æœåŠ¡ä¼˜åŒ–"
        
        try:
            pathfinding_path = self.workspace_path / "code" / "pathfinding"
            if not pathfinding_path.exists():
                return OptimizationResult(
                    name=name,
                    status=OptimizationStatus.SKIPPED,
                    message="è·¯å¾„è§„åˆ’ç›®å½•ä¸å­˜åœ¨",
                    duration=time.time() - start_time
                )
            
            # åˆ›å»ºè·¯å¾„è§„åˆ’æœåŠ¡é›†æˆè„šæœ¬
            integration_script = '''#!/usr/bin/env python3
"""
è·¯å¾„è§„åˆ’æœåŠ¡é›†æˆè„šæœ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from pathfinding.emergency_dispatcher import EmergencyDispatcher, create_sample_dispatcher
    from pathfinding.multi_objective_planner import MultiObjectivePlanner
    from pathfinding.shortest_path import ShortestPathFinder
    PATHFINDING_AVAILABLE = True
except ImportError as e:
    print(f"è·¯å¾„è§„åˆ’æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    PATHFINDING_AVAILABLE = False

class PathfindingService:
    """è·¯å¾„è§„åˆ’æœåŠ¡"""
    
    def __init__(self):
        self.dispatcher = None
        self.planner = None
        self.pathfinder = None
        self._initialize_services()
    
    def _initialize_services(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        if not PATHFINDING_AVAILABLE:
            print("è·¯å¾„è§„åˆ’æ¨¡å—ä¸å¯ç”¨")
            return
        
        try:
            # åˆå§‹åŒ–åº”æ€¥è°ƒåº¦å™¨
            self.dispatcher = create_sample_dispatcher()
            print("åº”æ€¥è°ƒåº¦å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–è·¯å¾„è§„åˆ’å™¨
            self.planner = MultiObjectivePlanner()
            print("å¤šç›®æ ‡è§„åˆ’å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–æœ€çŸ­è·¯å¾„æŸ¥æ‰¾å™¨
            self.pathfinder = ShortestPathFinder()
            print("æœ€çŸ­è·¯å¾„æŸ¥æ‰¾å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"è·¯å¾„è§„åˆ’æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def get_dashboard_data(self):
        """è·å–è°ƒåº¦é¢æ¿æ•°æ®"""
        if self.dispatcher:
            return self.dispatcher.get_dashboard_data()
        else:
            return {
                "center_name": "æ¨¡æ‹Ÿè°ƒåº¦ä¸­å¿ƒ",
                "total_vehicles": 5,
                "available_vehicles": 3,
                "busy_vehicles": 2,
                "pending_tasks": 1,
                "statistics": {
                    "total_dispatches": 10,
                    "successful_dispatches": 9,
                    "average_response_time": 8.5
                }
            }
    
    def dispatch_emergency_vehicle(self, request):
        """è°ƒåº¦åº”æ€¥è½¦è¾†"""
        if self.dispatcher:
            try:
                # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„è°ƒåº¦é€»è¾‘
                return {
                    "dispatch_id": f"DISP_{int(time.time())}",
                    "status": "success",
                    "estimated_arrival": "8åˆ†é’Ÿ",
                    "vehicle_assigned": "AMB_001"
                }
            except Exception as e:
                print(f"è°ƒåº¦å¤±è´¥: {e}")
        
        # è¿”å›æ¨¡æ‹Ÿç»“æœ
        return {
            "dispatch_id": f"DISP_{int(time.time())}",
            "status": "mock_success",
            "estimated_arrival": "10åˆ†é’Ÿ",
            "vehicle_assigned": "MOCK_VEHICLE_001"
        }

# å…¨å±€è·¯å¾„è§„åˆ’æœåŠ¡å®ä¾‹
pathfinding_service = PathfindingService()
'''
            
            integration_script_path = pathfinding_path / "service_integration.py"
            with open(integration_script_path, 'w', encoding='utf-8') as f:
                f.write(integration_script)
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message="è·¯å¾„è§„åˆ’æœåŠ¡é›†æˆè„šæœ¬å·²åˆ›å»º",
                duration=time.time() - start_time,
                details={"integration_script": str(integration_script_path)}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"è·¯å¾„è§„åˆ’æœåŠ¡ä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def optimize_data_flow(self) -> OptimizationResult:
        """ä¼˜åŒ–æ•°æ®æµ"""
        start_time = time.time()
        name = "æ•°æ®æµä¼˜åŒ–"
        
        try:
            # åˆ›å»ºæ•°æ®æµé…ç½®
            data_flow_config = {
                "data_sources": {
                    "traffic_sensors": {
                        "type": "iot_sensors",
                        "frequency": "5min",
                        "endpoints": ["/api/realtime"],
                        "cache_ttl": 300
                    },
                    "weather_api": {
                        "type": "external_api",
                        "frequency": "15min",
                        "endpoints": ["/api/weather"],
                        "cache_ttl": 900
                    },
                    "emergency_systems": {
                        "type": "internal_api",
                        "frequency": "realtime",
                        "endpoints": ["/api/emergency/vehicles"],
                        "cache_ttl": 60
                    }
                },
                "data_processing": {
                    "real_time_pipeline": {
                        "enabled": True,
                        "batch_size": 100,
                        "processing_interval": 30
                    },
                    "prediction_pipeline": {
                        "enabled": True,
                        "model_update_interval": 3600,
                        "prediction_horizon": 1800
                    }
                },
                "data_storage": {
                    "cache": {
                        "type": "memory",
                        "max_size": "1GB",
                        "ttl": 3600
                    },
                    "database": {
                        "type": "postgresql",
                        "connection_pool": 10,
                        "retention_period": "30days"
                    }
                },
                "data_quality": {
                    "validation": {
                        "enabled": True,
                        "rules": ["range_check", "null_check", "consistency_check"]
                    },
                    "monitoring": {
                        "enabled": True,
                        "alert_threshold": 0.05
                    }
                }
            }
            
            config_path = self.workspace_path / "code" / "data_flow_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(data_flow_config, f, indent=2, ensure_ascii=False)
            
            # åˆ›å»ºæ•°æ®æµç›‘æ§è„šæœ¬
            monitoring_script = '''#!/usr/bin/env python3
"""
æ•°æ®æµç›‘æ§è„šæœ¬
"""

import time
import json
import logging
from datetime import datetime, timedelta

class DataFlowMonitor:
    """æ•°æ®æµç›‘æ§å™¨"""
    
    def __init__(self, config_path):
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        self.logger = logging.getLogger(__name__)
        self.metrics = {
            "data_received": 0,
            "data_processed": 0,
            "errors": 0,
            "last_update": None
        }
    
    def monitor_data_flow(self):
        """ç›‘æ§æ•°æ®æµ"""
        print("å¼€å§‹æ•°æ®æµç›‘æ§...")
        
        while True:
            try:
                # æ¨¡æ‹Ÿæ•°æ®æ¥æ”¶
                self.metrics["data_received"] += 1
                self.metrics["last_update"] = datetime.now().isoformat()
                
                # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
                if self.metrics["data_received"] % 10 == 0:
                    self.metrics["data_processed"] += 1
                
                # æ‰“å°ç›‘æ§ä¿¡æ¯
                if self.metrics["data_received"] % 20 == 0:
                    print(f"ç›‘æ§æ•°æ®: {self.metrics}")
                
                time.sleep(1)
                
            except KeyboardInterrupt:
                print("æ•°æ®æµç›‘æ§åœæ­¢")
                break
            except Exception as e:
                self.metrics["errors"] += 1
                print(f"ç›‘æ§é”™è¯¯: {e}")
                time.sleep(5)

if __name__ == "__main__":
    monitor = DataFlowMonitor("data_flow_config.json")
    monitor.monitor_data_flow()
'''
            
            monitoring_script_path = self.workspace_path / "code" / "data_flow_monitor.py"
            with open(monitoring_script_path, 'w', encoding='utf-8') as f:
                f.write(monitoring_script)
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message="æ•°æ®æµé…ç½®å’Œç›‘æ§è„šæœ¬å·²åˆ›å»º",
                duration=time.time() - start_time,
                details={"config_file": str(config_path), "monitor_script": str(monitoring_script_path)}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"æ•°æ®æµä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def optimize_performance(self) -> OptimizationResult:
        """ä¼˜åŒ–æ€§èƒ½"""
        start_time = time.time()
        name = "æ€§èƒ½ä¼˜åŒ–"
        
        try:
            # åˆ›å»ºæ€§èƒ½é…ç½®
            performance_config = {
                "caching": {
                    "enabled": True,
                    "strategies": {
                        "api_responses": {
                            "ttl": 300,
                            "max_size": "100MB"
                        },
                        "model_predictions": {
                            "ttl": 600,
                            "max_size": "500MB"
                        },
                        "static_data": {
                            "ttl": 3600,
                            "max_size": "50MB"
                        }
                    }
                },
                "compression": {
                    "enabled": True,
                    "algorithms": ["gzip", "brotli"],
                    "min_size": 1024
                },
                "load_balancing": {
                    "enabled": False,  # å•æœºéƒ¨ç½²
                    "strategies": ["round_robin", "least_connections"]
                },
                "resource_limits": {
                    "cpu": "80%",
                    "memory": "85%",
                    "disk": "90%"
                },
                "optimization": {
                    "connection_pooling": True,
                    "query_optimization": True,
                    "batch_processing": True,
                    "async_processing": True
                }
            }
            
            config_path = self.workspace_path / "code" / "performance_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(performance_config, f, indent=2, ensure_ascii=False)
            
            # åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬
            performance_test_script = '''#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•è„šæœ¬
"""

import time
import statistics
import requests
import concurrent.futures
from typing import List, Dict

class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:3001"):
        self.base_url = base_url
        self.results = []
    
    def test_api_response_time(self, endpoint: str, num_requests: int = 10) -> Dict:
        """æµ‹è¯•APIå“åº”æ—¶é—´"""
        response_times = []
        
        for i in range(num_requests):
            start_time = time.time()
            try:
                response = requests.get(f"{self.base_url}{endpoint}", timeout=30)
                end_time = time.time()
                
                if response.status_code == 200:
                    response_times.append(end_time - start_time)
                else:
                    print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                    
            except Exception as e:
                print(f"è¯·æ±‚å¼‚å¸¸: {e}")
        
        if response_times:
            return {
                "endpoint": endpoint,
                "total_requests": num_requests,
                "successful_requests": len(response_times),
                "avg_response_time": statistics.mean(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "median_response_time": statistics.median(response_times)
            }
        else:
            return {"endpoint": endpoint, "error": "æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†"}
    
    def test_concurrent_requests(self, endpoint: str, concurrent_count: int = 5) -> Dict:
        """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
        def make_request():
            start_time = time.time()
            try:
                response = requests.get(f"{self.base_url}{endpoint}", timeout=30)
                end_time = time.time()
                return end_time - start_time if response.status_code == 200 else None
            except:
                return None
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_count) as executor:
            futures = [executor.submit(make_request) for _ in range(concurrent_count * 2)]
            response_times = [future.result() for future in concurrent_futures.as_completed(futures) if future.result() is not None]
        
        if response_times:
            return {
                "endpoint": endpoint,
                "concurrent_requests": concurrent_count * 2,
                "successful_requests": len(response_times),
                "avg_concurrent_response_time": statistics.mean(response_times),
                "throughput": len(response_times) / max(response_times)
            }
        else:
            return {"endpoint": endpoint, "error": "å¹¶å‘è¯·æ±‚éƒ½å¤±è´¥äº†"}
    
    def run_performance_tests(self) -> List[Dict]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        endpoints = [
            "/api/health",
            "/api/realtime",
            "/api/system/metrics"
        ]
        
        results = []
        
        for endpoint in endpoints:
            print(f"æµ‹è¯•ç«¯ç‚¹: {endpoint}")
            
            # å“åº”æ—¶é—´æµ‹è¯•
            response_time_result = self.test_api_response_time(endpoint)
            results.append(response_time_result)
            
            # å¹¶å‘è¯·æ±‚æµ‹è¯•
            concurrent_result = self.test_concurrent_requests(endpoint)
            results.append(concurrent_result)
        
        return results

if __name__ == "__main__":
    tester = PerformanceTester()
    results = tester.run_performance_tests()
    
    print("\\næ€§èƒ½æµ‹è¯•ç»“æœ:")
    for result in results:
        print(json.dumps(result, indent=2, ensure_ascii=False))
'''
            
            performance_test_script_path = self.workspace_path / "code" / "performance_test.py"
            with open(performance_test_script_path, 'w', encoding='utf-8') as f:
                f.write(performance_test_script)
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message="æ€§èƒ½é…ç½®å’Œæµ‹è¯•è„šæœ¬å·²åˆ›å»º",
                duration=time.time() - start_time,
                details={"config_file": str(config_path), "test_script": str(performance_test_script_path)}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def optimize_error_handling(self) -> OptimizationResult:
        """ä¼˜åŒ–é”™è¯¯å¤„ç†"""
        start_time = time.time()
        name = "é”™è¯¯å¤„ç†ä¼˜åŒ–"
        
        try:
            # åˆ›å»ºé”™è¯¯å¤„ç†é…ç½®
            error_handling_config = {
                "global_exception_handler": {
                    "enabled": True,
                    "log_level": "ERROR",
                    "notification": {
                        "email": False,
                        "webhook": False
                    }
                },
                "api_error_handling": {
                    "retry_attempts": 3,
                    "retry_delay": 1.0,
                    "fallback_responses": {
                        "health_check": {"status": "degraded", "message": "æœåŠ¡é™çº§è¿è¡Œ"},
                        "realtime_data": {"success": False, "error": "æ•°æ®æš‚æ—¶ä¸å¯ç”¨"},
                        "prediction": {"success": False, "error": "é¢„æµ‹æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"}
                    }
                },
                "model_error_handling": {
                    "timeout": 30.0,
                    "fallback_to_mock": True,
                    "error_logging": True
                },
                "data_validation": {
                    "enabled": True,
                    "strict_mode": False,
                    "sanitize_inputs": True
                },
                "circuit_breaker": {
                    "enabled": True,
                    "failure_threshold": 5,
                    "recovery_timeout": 60,
                    "half_open_max_calls": 3
                }
            }
            
            config_path = self.workspace_path / "code" / "error_handling_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(error_handling_config, f, indent=2, ensure_ascii=False)
            
            # åˆ›å»ºé”™è¯¯å¤„ç†ä¸­é—´ä»¶
            error_middleware = '''"""
å…¨å±€é”™è¯¯å¤„ç†ä¸­é—´ä»¶
"""

import logging
import traceback
import time
from functools import wraps
from typing import Any, Callable

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CircuitBreaker:
    """ç†”æ–­å™¨"""
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
    
    def call(self, func: Callable, *args, **kwargs) -> Any:
        """è°ƒç”¨å‡½æ•°"""
        if self.state == 'OPEN':
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = 'HALF_OPEN'
            else:
                raise Exception("ç†”æ–­å™¨å¼€å¯ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        
        try:
            result = func(*args, **kwargs)
            if self.state == 'HALF_OPEN':
                self.state = 'CLOSED'
                self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = 'OPEN'
            
            raise e

# å…¨å±€ç†”æ–­å™¨å®ä¾‹
circuit_breaker = CircuitBreaker()

def global_exception_handler(func: Callable) -> Callable:
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"å…¨å±€å¼‚å¸¸: {func.__name__} - {str(e)}")
            logger.error(traceback.format_exc())
            
            # æ ¹æ®å‡½æ•°åè¿”å›åˆé€‚çš„é™çº§å“åº”
            if func.__name__ == 'health_check':
                return {"status": "error", "message": "æœåŠ¡å¼‚å¸¸"}
            elif func.__name__ == 'get_realtime_data':
                return {"success": False, "error": "æ•°æ®è·å–å¤±è´¥"}
            else:
                raise e
    
    return wrapper

def api_error_handler(fallback_response: dict = None):
    """APIé”™è¯¯å¤„ç†å™¨è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return circuit_breaker.call(func, *args, **kwargs)
            except Exception as e:
                logger.error(f"APIé”™è¯¯: {func.__name__} - {str(e)}")
                
                if fallback_response:
                    return fallback_response
                else:
                    return {"success": False, "error": str(e)}
        
        return wrapper
    return decorator

# å¯¼å‡ºè£…é¥°å™¨
api_error_handler_decorator = api_error_handler
global_exception_handler_decorator = global_exception_handler
'''
            
            middleware_path = self.workspace_path / "code" / "error_middleware.py"
            with open(middleware_path, 'w', encoding='utf-8') as f:
                f.write(error_middleware)
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message="é”™è¯¯å¤„ç†é…ç½®å’Œä¸­é—´ä»¶å·²åˆ›å»º",
                duration=time.time() - start_time,
                details={"config_file": str(config_path), "middleware_file": str(middleware_path)}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"é”™è¯¯å¤„ç†ä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def optimize_monitoring(self) -> OptimizationResult:
        """ä¼˜åŒ–ç›‘æ§"""
        start_time = time.time()
        name = "ç›‘æ§ä¼˜åŒ–"
        
        try:
            # åˆ›å»ºç›‘æ§é…ç½®
            monitoring_config = {
                "metrics": {
                    "system": {
                        "cpu_usage": {"enabled": True, "interval": 30},
                        "memory_usage": {"enabled": True, "interval": 30},
                        "disk_usage": {"enabled": True, "interval": 60},
                        "network_io": {"enabled": True, "interval": 30}
                    },
                    "application": {
                        "request_count": {"enabled": True, "interval": 10},
                        "response_time": {"enabled": True, "interval": 10},
                        "error_rate": {"enabled": True, "interval": 30},
                        "active_connections": {"enabled": True, "interval": 10}
                    },
                    "business": {
                        "prediction_accuracy": {"enabled": True, "interval": 300},
                        "emergency_response_time": {"enabled": True, "interval": 60},
                        "system_availability": {"enabled": True, "interval": 60}
                    }
                },
                "alerts": {
                    "cpu_high": {"threshold": 80, "duration": 300},
                    "memory_high": {"threshold": 85, "duration": 300},
                    "response_time_high": {"threshold": 10, "duration": 120},
                    "error_rate_high": {"threshold": 0.05, "duration": 180}
                },
                "logging": {
                    "level": "INFO",
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                    "handlers": ["console", "file"],
                    "file_path": "logs/system.log",
                    "max_size": "100MB",
                    "backup_count": 5
                }
            }
            
            config_path = self.workspace_path / "code" / "monitoring_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(monitoring_config, f, indent=2, ensure_ascii=False)
            
            # åˆ›å»ºç›‘æ§è„šæœ¬
            monitoring_script = '''#!/usr/bin/env python3
"""
ç³»ç»Ÿç›‘æ§è„šæœ¬
"""

import psutil
import time
import json
import logging
from datetime import datetime
from typing import Dict, List

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, self.config['logging']['level']),
            format=self.config['logging']['format']
        )
        self.logger = logging.getLogger(__name__)
        
        self.metrics_history = []
    
    def collect_system_metrics(self) -> Dict:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "network_io": dict(psutil.net_io_counters()._asdict()),
            "process_count": len(psutil.pids()),
            "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0]
        }
    
    def collect_application_metrics(self) -> Dict:
        """æ”¶é›†åº”ç”¨æŒ‡æ ‡"""
        # è¿™é‡Œåº”è¯¥æ”¶é›†å®é™…çš„åº”ç”¨æŒ‡æ ‡
        # ç°åœ¨ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        return {
            "timestamp": datetime.now().isoformat(),
            "request_count": 0,  # TODO: ä»å®é™…åº”ç”¨ä¸­è·å–
            "response_time_avg": 0.0,  # TODO: ä»å®é™…åº”ç”¨ä¸­è·å–
            "error_rate": 0.0,  # TODO: ä»å®é™…åº”ç”¨ä¸­è·å–
            "active_connections": 0  # TODO: ä»å®é™…åº”ç”¨ä¸­è·å–
        }
    
    def check_alerts(self, metrics: Dict) -> List[Dict]:
        """æ£€æŸ¥å‘Šè­¦"""
        alerts = []
        
        # CPUä½¿ç”¨ç‡å‘Šè­¦
        if metrics['system']['cpu_percent'] > self.config['alerts']['cpu_high']['threshold']:
            alerts.append({
                "type": "cpu_high",
                "message": f"CPUä½¿ç”¨ç‡è¿‡é«˜: {metrics['system']['cpu_percent']:.1f}%",
                "timestamp": datetime.now().isoformat()
            })
        
        # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
        if metrics['system']['memory_percent'] > self.config['alerts']['memory_high']['threshold']:
            alerts.append({
                "type": "memory_high",
                "message": f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {metrics['system']['memory_percent']:.1f}%",
                "timestamp": datetime.now().isoformat()
            })
        
        return alerts
    
    def run_monitoring(self):
        """è¿è¡Œç›‘æ§"""
        self.logger.info("å¼€å§‹ç³»ç»Ÿç›‘æ§...")
        
        try:
            while True:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                system_metrics = self.collect_system_metrics()
                
                # æ”¶é›†åº”ç”¨æŒ‡æ ‡
                application_metrics = self.collect_application_metrics()
                
                # åˆå¹¶æŒ‡æ ‡
                all_metrics = {
                    "system": system_metrics,
                    "application": application_metrics
                }
                
                # æ£€æŸ¥å‘Šè­¦
                alerts = self.check_alerts(all_metrics)
                
                # è®°å½•æŒ‡æ ‡
                self.metrics_history.append(all_metrics)
                
                # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-1000:]
                
                # è¾“å‡ºå‘Šè­¦
                for alert in alerts:
                    self.logger.warning(f"å‘Šè­¦: {alert['message']}")
                
                # å®šæœŸè¾“å‡ºçŠ¶æ€
                if len(self.metrics_history) % 10 == 0:
                    self.logger.info(f"ç³»ç»ŸçŠ¶æ€: CPU={system_metrics['cpu_percent']:.1f}%, "
                                   f"å†…å­˜={system_metrics['memory_percent']:.1f}%, "
                                   f"ç£ç›˜={system_metrics['disk_usage']:.1f}%")
                
                time.sleep(30)  # 30ç§’ç›‘æ§é—´éš”
                
        except KeyboardInterrupt:
            self.logger.info("ç›‘æ§åœæ­¢")
        except Exception as e:
            self.logger.error(f"ç›‘æ§å¼‚å¸¸: {e}")

if __name__ == "__main__":
    monitor = SystemMonitor("monitoring_config.json")
    monitor.run_monitoring()
'''
            
            monitoring_script_path = self.workspace_path / "code" / "system_monitor.py"
            with open(monitoring_script_path, 'w', encoding='utf-8') as f:
                f.write(monitoring_script)
            
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.SUCCESS,
                message="ç›‘æ§é…ç½®å’Œè„šæœ¬å·²åˆ›å»º",
                duration=time.time() - start_time,
                details={"config_file": str(config_path), "monitor_script": str(monitoring_script_path)}
            )
            
        except Exception as e:
            return OptimizationResult(
                name=name,
                status=OptimizationStatus.FAILED,
                message=f"ç›‘æ§ä¼˜åŒ–å¤±è´¥: {str(e)}",
                duration=time.time() - start_time
            )
    
    def generate_optimization_report(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report = []
        report.append("# ç³»ç»Ÿé›†æˆä¼˜åŒ–æŠ¥å‘Š")
        report.append(f"**ä¼˜åŒ–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**ä¼˜åŒ–é¡¹ç›®æ•°**: {len(self.results)}")
        report.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        success_count = sum(1 for r in self.results if r.status == OptimizationStatus.SUCCESS)
        failed_count = sum(1 for r in self.results if r.status == OptimizationStatus.FAILED)
        skipped_count = sum(1 for r in self.results if r.status == OptimizationStatus.SKIPPED)
        
        report.append("## ä¼˜åŒ–ç»Ÿè®¡")
        report.append(f"- âœ… æˆåŠŸ: {success_count}")
        report.append(f"- âŒ å¤±è´¥: {failed_count}")
        report.append(f"- â­ï¸ è·³è¿‡: {skipped_count}")
        report.append(f"- ğŸ“Š æˆåŠŸç‡: {(success_count / len(self.results) * 100):.1f}%")
        report.append("")
        
        # è¯¦ç»†ç»“æœ
        report.append("## ä¼˜åŒ–è¯¦æƒ…")
        for result in self.results:
            status_emoji = {
                OptimizationStatus.SUCCESS: "âœ…",
                OptimizationStatus.FAILED: "âŒ",
                OptimizationStatus.SKIPPED: "â­ï¸",
                OptimizationStatus.RUNNING: "ğŸ”„",
                OptimizationStatus.PENDING: "â³"
            }
            
            report.append(f"### {status_emoji[result.status]} {result.name}")
            report.append(f"**çŠ¶æ€**: {result.status.value}")
            report.append(f"**è€—æ—¶**: {result.duration:.2f}ç§’")
            report.append(f"**æ¶ˆæ¯**: {result.message}")
            
            if result.details:
                report.append("**è¯¦ç»†ä¿¡æ¯**:")
                for key, value in result.details.items():
                    report.append(f"- {key}: {value}")
            
            report.append("")
        
        # å»ºè®®
        report.append("## åç»­å»ºè®®")
        if failed_count > 0:
            report.append("1. æ£€æŸ¥å¤±è´¥çš„ä¼˜åŒ–é¡¹ç›®ï¼ŒæŸ¥çœ‹é”™è¯¯æ—¥å¿—")
            report.append("2. ç¡®ä¿æ‰€æœ‰ä¾èµ–é¡¹å·²æ­£ç¡®å®‰è£…")
            report.append("3. éªŒè¯é…ç½®æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„")
        
        if success_count > 0:
            report.append("4. è¿è¡Œé›†æˆæµ‹è¯•éªŒè¯ä¼˜åŒ–æ•ˆæœ")
            report.append("5. å¯åŠ¨ä¼˜åŒ–åçš„æœåŠ¡è¿›è¡Œæµ‹è¯•")
            report.append("6. ç›‘æ§ç³»ç»Ÿæ€§èƒ½å’Œç¨³å®šæ€§")
        
        report.append("")
        report.append("---")
        report.append("*æ­¤æŠ¥å‘Šç”±ç³»ç»Ÿé›†æˆä¼˜åŒ–å™¨è‡ªåŠ¨ç”Ÿæˆ*")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    print("æ™ºèƒ½äº¤é€šæµé¢„æµ‹ç³»ç»Ÿé›†æˆä¼˜åŒ–å™¨")
    print("=" * 50)
    
    optimizer = IntegrationOptimizer()
    results = optimizer.run_optimization()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = optimizer.generate_optimization_report()
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path("/workspace/code/integration/optimization_report.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # è¾“å‡ºæ‘˜è¦
    success_count = sum(1 for r in results if r.status == OptimizationStatus.SUCCESS)
    print(f"\nä¼˜åŒ–å®Œæˆ! æˆåŠŸ: {success_count}/{len(results)}")


if __name__ == "__main__":
    main()
