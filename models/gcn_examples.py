"""
GCNç½‘ç»œä½¿ç”¨ç¤ºä¾‹

è¯¥æ–‡ä»¶æä¾›äº†GCNç½‘ç»œåœ¨å®é™…åº”ç”¨ä¸­çš„ä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
2. äº¤é€šæµé¢„æµ‹ç¤ºä¾‹
3. ä¸åŒé…ç½®çš„å¯¹æ¯”ç¤ºä¾‹
4. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç¤ºä¾‹

Author: AI Assistant
Date: 2025-11-05
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
import json
import pickle

from gcn_network import (
    GraphDataProcessor,
    GCNNetwork,
    GCNTrainer,
    GCNEvaluator,
    create_sample_data
)


def basic_usage_example():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("=== GCNç½‘ç»œåŸºç¡€ä½¿ç”¨ç¤ºä¾‹ ===\n")
    
    # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("1. åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    data, coordinates = create_sample_data(
        n_timesteps=500, 
        n_nodes=30, 
        n_features=1
    )
    print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"   åæ ‡å½¢çŠ¶: {coordinates.shape}")
    
    # 2. æ•°æ®é¢„å¤„ç†
    print("\n2. æ•°æ®é¢„å¤„ç†...")
    processor = GraphDataProcessor(
        normalization='zscore',
        adj_threshold=0.1,
        sigma2=1.0,
        epsilon=0.1
    )
    
    graph_data = processor.prepare_graph_data(
        data, coordinates, 
        window_size=12, 
        prediction_steps=3
    )
    print(f"   è¾“å…¥æ•°æ®: {graph_data['X'].shape}")
    print(f"   ç›®æ ‡æ•°æ®: {graph_data['y'].shape}")
    print(f"   é‚»æ¥çŸ©é˜µ: {graph_data['adj_matrix'].shape}")
    
    # 3. æ•°æ®é›†åˆ’åˆ†
    print("\n3. æ•°æ®é›†åˆ’åˆ†...")
    X = graph_data['X']
    y = graph_data['y']
    
    n_samples = X.shape[0]
    train_size = int(0.6 * n_samples)
    val_size = int(0.2 * n_samples)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train = torch.FloatTensor(X[:train_size])
    y_train = torch.FloatTensor(y[:train_size])
    X_val = torch.FloatTensor(X[train_size:train_size+val_size])
    y_val = torch.FloatTensor(y[train_size:train_size+val_size])
    X_test = torch.FloatTensor(X[train_size+val_size:])
    y_test = torch.FloatTensor(y[train_size+val_size:])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)
    test_dataset = torch.utils.data.TensorDataset(X_test, y_test)
    
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("\n4. åˆ›å»ºGCNæ¨¡å‹...")
    model = GCNNetwork(
        n_nodes=graph_data['n_nodes'],
        n_features=graph_data['n_features'],
        n_hidden=64,
        n_layers=3,
        conv_type='cheb',  # å¯é€‰: 'gcn', 'cheb', 'sage', 'gat'
        prediction_steps=graph_data['prediction_steps'],
        use_attention=True,
        use_dynamic_adj=True,
        dropout=0.1
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # 5. è®­ç»ƒæ¨¡å‹
    print("\n5. è®­ç»ƒæ¨¡å‹...")
    trainer = GCNTrainer(model, learning_rate=0.001)
    
    # å‡†å¤‡å›¾æ•°æ®ï¼ˆè½¬æ¢ä¸ºå¼ é‡ï¼‰
    graph_data_tensor = {
        'adj_matrix': torch.FloatTensor(graph_data['adj_matrix']),
        'laplacian': torch.FloatTensor(graph_data['laplacian']),
        'coordinates': torch.FloatTensor(graph_data['coordinates'])
    }
    
    # è®­ç»ƒæ¨¡å‹
    training_history = trainer.train(
        train_loader, 
        val_loader, 
        graph_data_tensor,
        epochs=30,
        patience=10
    )
    
    # 6. è¯„ä¼°æ¨¡å‹
    print("\n6. è¯„ä¼°æ¨¡å‹...")
    evaluator = GCNEvaluator(model)
    results = evaluator.evaluate(test_loader, graph_data_tensor)
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print("   è¯„ä¼°æŒ‡æ ‡:")
    for metric, value in results['metrics'].items():
        print(f"     {metric}: {value:.6f}")
    
    # 7. å¯è§†åŒ–ç»“æœ
    print("\n7. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    evaluator.plot_results(results, save_path='basic_example_results.png')
    
    # 8. ä¿å­˜æ¨¡å‹
    print("\n8. ä¿å­˜æ¨¡å‹...")
    model_save_path = 'basic_gcn_model.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': {
            'n_nodes': graph_data['n_nodes'],
            'n_features': graph_data['n_features'],
            'n_hidden': 64,
            'n_layers': 3,
            'conv_type': 'cheb',
            'prediction_steps': graph_data['prediction_steps'],
            'use_attention': True,
            'use_dynamic_adj': True,
            'dropout': 0.1
        },
        'training_history': training_history,
        'metrics': results['metrics']
    }, model_save_path)
    print(f"   æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
    
    return model, results, training_history


def traffic_prediction_example():
    """äº¤é€šæµé¢„æµ‹ç¤ºä¾‹"""
    print("\n=== äº¤é€šæµé¢„æµ‹ç¤ºä¾‹ ===\n")
    
    # æ¨¡æ‹Ÿæ›´çœŸå®çš„äº¤é€šæ•°æ®
    np.random.seed(42)
    
    # 1. åˆ›å»ºè·¯ç½‘æ‹“æ‰‘ï¼ˆæ¨¡æ‹ŸåŸå¸‚é“è·¯ç½‘ç»œï¼‰
    n_nodes = 50
    coordinates = np.random.uniform(0, 1000, (n_nodes, 2))  # åŸå¸‚åŒºåŸŸ 1km x 1km
    
    # åˆ›å»ºæ›´çœŸå®çš„é‚»æ¥çŸ©é˜µï¼ˆåŸºäºè·ç¦»å’Œè¿æ¥æ€§ï¼‰
    def create_road_network(coordinates, connection_prob=0.1):
        n_nodes = len(coordinates)
        adj_matrix = np.zeros((n_nodes, n_nodes))
        
        # åŸºäºè·ç¦»çš„è¿æ¥
        distances = np.linalg.norm(
            coordinates[:, np.newaxis] - coordinates[np.newaxis, :], axis=2
        )
        
        # è·ç¦»é˜ˆå€¼
        distance_threshold = 200  # 200ç±³
        
        for i in range(n_nodes):
            for j in range(i+1, n_nodes):
                if distances[i, j] < distance_threshold:
                    # åŸºäºè·ç¦»çš„æƒé‡
                    weight = np.exp(-distances[i, j] / 100)
                    if np.random.random() < connection_prob:
                        adj_matrix[i, j] = weight
                        adj_matrix[j, i] = weight
        
        return adj_matrix
    
    adj_matrix = create_road_network(coordinates)
    
    # 2. ç”Ÿæˆäº¤é€šæµæ•°æ®
    def generate_traffic_data(n_timesteps, n_nodes, adj_matrix):
        """ç”Ÿæˆæ¨¡æ‹Ÿäº¤é€šæµæ•°æ®"""
        data = np.zeros((n_timesteps, n_nodes, 3))  # é€Ÿåº¦ã€æµé‡ã€å ç”¨ç‡
        
        # æ¨¡æ‹Ÿä¸€å¤©24å°æ—¶ * 7å¤© = 168ä¸ªæ—¶é—´ç‚¹ï¼ˆæ¯10åˆ†é’Ÿä¸€ä¸ªç‚¹ï¼‰
        time_of_day = np.arange(n_timesteps) % 168
        
        for node in range(n_nodes):
            # åŸºäºæ—¶é—´çš„åŸºç¡€æ¨¡å¼
            base_speed = 60 + 20 * np.sin(2 * np.pi * time_of_day / 168)  # é€Ÿåº¦å˜åŒ–
            base_flow = 1000 + 500 * np.sin(2 * np.pi * time_of_day / 168 + np.pi/4)  # æµé‡å˜åŒ–
            base_occupancy = 0.3 + 0.2 * np.sin(2 * np.pi * time_of_day / 168 + np.pi/2)  # å ç”¨ç‡å˜åŒ–
            
            # æ·»åŠ é‚»æ¥èŠ‚ç‚¹çš„å½±å“
            neighbors = np.where(adj_matrix[node] > 0)[0]
            if len(neighbors) > 0:
                neighbor_effect = np.mean([base_speed[neighbor] for neighbor in neighbors], axis=0)
                base_speed = 0.7 * base_speed + 0.3 * neighbor_effect
            
            # æ·»åŠ å™ªå£°
            speed_noise = np.random.normal(0, 5, n_timesteps)
            flow_noise = np.random.normal(0, 50, n_timesteps)
            occupancy_noise = np.random.normal(0, 0.05, n_timesteps)
            
            # ç¡®ä¿å€¼åœ¨åˆç†èŒƒå›´å†…
            speed = np.clip(base_speed + speed_noise, 10, 120)  # 10-120 km/h
            flow = np.clip(base_flow + flow_noise, 0, 2000)  # 0-2000 veh/h
            occupancy = np.clip(base_occupancy + occupancy_noise, 0.05, 0.95)  # 5%-95%
            
            data[:, node, 0] = speed
            data[:, node, 1] = flow
            data[:, node, 2] = occupancy
        
        return data
    
    # ç”Ÿæˆä¸€å‘¨çš„äº¤é€šæ•°æ®
    n_timesteps = 168 * 7  # ä¸€å‘¨ï¼Œæ¯10åˆ†é’Ÿä¸€ä¸ªæ•°æ®ç‚¹
    traffic_data = generate_traffic_data(n_timesteps, n_nodes, adj_matrix)
    
    print(f"1. ç”Ÿæˆäº¤é€šæ•°æ®: {traffic_data.shape}")
    print(f"   é€Ÿåº¦èŒƒå›´: {traffic_data[:,:,0].min():.1f} - {traffic_data[:,:,0].max():.1f} km/h")
    print(f"   æµé‡èŒƒå›´: {traffic_data[:,:,1].min():.0f} - {traffic_data[:,:,1].max():.0f} veh/h")
    print(f"   å ç”¨ç‡èŒƒå›´: {traffic_data[:,:,2].min():.2f} - {traffic_data[:,:,2].max():.2f}")
    
    # 3. æ•°æ®é¢„å¤„ç†
    print("\n2. æ•°æ®é¢„å¤„ç†...")
    processor = GraphDataProcessor(normalization='zscore')
    
    # ä½¿ç”¨è‡ªå®šä¹‰é‚»æ¥çŸ©é˜µ
    graph_data = processor.prepare_graph_data(traffic_data, coordinates, window_size=24, prediction_steps=6)
    
    # æ›¿æ¢ä¸ºè‡ªå®šä¹‰çš„é‚»æ¥çŸ©é˜µ
    graph_data['adj_matrix'] = adj_matrix
    graph_data['laplacian'] = processor._compute_laplacian(adj_matrix)
    
    print(f"   è¾“å…¥æ•°æ®: {graph_data['X'].shape}")
    print(f"   é¢„æµ‹æ­¥æ•°: {graph_data['prediction_steps']} (ç›¸å½“äº {graph_data['prediction_steps'] * 10} åˆ†é’Ÿ)")
    
    # 4. æ•°æ®é›†åˆ’åˆ†ï¼ˆæ—¶é—´åºåˆ—åˆ’åˆ†ï¼‰
    X = graph_data['X']
    y = graph_data['y']
    
    n_samples = X.shape[0]
    train_size = int(0.6 * n_samples)
    val_size = int(0.2 * n_samples)
    
    X_train = torch.FloatTensor(X[:train_size])
    y_train = torch.FloatTensor(y[:train_size])
    X_val = torch.FloatTensor(X[train_size:train_size+val_size])
    y_val = torch.FloatTensor(y[train_size:val_size+val_size])
    X_test = torch.FloatTensor(X[train_size+val_size:])
    y_test = torch.FloatTensor(y[train_size+val_size:])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)
    test_dataset = torch.utils.data.TensorDataset(X_test, y_test)
    
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)
    
    # 5. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
    print("\n3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹...")
    model = GCNNetwork(
        n_nodes=graph_data['n_nodes'],
        n_features=graph_data['n_features'],  # 3: é€Ÿåº¦ã€æµé‡ã€å ç”¨ç‡
        n_hidden=128,
        n_layers=4,
        conv_type='sage',  # GraphSAGEå¯¹äº¤é€šæ•°æ®æ•ˆæœè¾ƒå¥½
        prediction_steps=graph_data['prediction_steps'],
        use_attention=True,
        use_dynamic_adj=True,
        dropout=0.2
    )
    
    trainer = GCNTrainer(model, learning_rate=0.0005)
    
    graph_data_tensor = {
        'adj_matrix': torch.FloatTensor(graph_data['adj_matrix']),
        'laplacian': torch.FloatTensor(graph_data['laplacian']),
        'coordinates': torch.FloatTensor(graph_data['coordinates'])
    }
    
    # è®­ç»ƒæ¨¡å‹
    training_history = trainer.train(
        train_loader, 
        val_loader, 
        graph_data_tensor,
        epochs=50,
        patience=15
    )
    
    # 6. è¯„ä¼°æ¨¡å‹
    print("\n4. è¯„ä¼°æ¨¡å‹...")
    evaluator = GCNEvaluator(model)
    results = evaluator.evaluate(test_loader, graph_data_tensor)
    
    # åˆ†åˆ«è¯„ä¼°ä¸åŒç‰¹å¾çš„é¢„æµ‹æ€§èƒ½
    feature_names = ['é€Ÿåº¦', 'æµé‡', 'å ç”¨ç‡']
    print("   åˆ†ç‰¹å¾è¯„ä¼°ç»“æœ:")
    for i, feature_name in enumerate(feature_names):
        pred_feature = results['predictions'][:, :, :, i]
        target_feature = results['targets'][:, :, :, i]
        
        mae = np.mean(np.abs(pred_feature - target_feature))
        rmse = np.sqrt(np.mean((pred_feature - target_feature) ** 2))
        
        print(f"     {feature_name}: MAE = {mae:.4f}, RMSE = {rmse:.4f}")
    
    # 7. å¯è§†åŒ–è·¯ç½‘å’Œé¢„æµ‹ç»“æœ
    print("\n5. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    
    # è·¯ç½‘å¯è§†åŒ–
    plt.figure(figsize=(15, 5))
    
    # è·¯ç½‘æ‹“æ‰‘
    plt.subplot(1, 3, 1)
    plt.scatter(coordinates[:, 0], coordinates[:, 1], c='red', s=30, alpha=0.7)
    
    # ç»˜åˆ¶è¾¹
    for i in range(n_nodes):
        for j in range(i+1, n_nodes):
            if adj_matrix[i, j] > 0:
                plt.plot([coordinates[i, 0], coordinates[j, 0]], 
                        [coordinates[i, 1], coordinates[j, 1]], 
                        'b-', alpha=0.3, linewidth=0.5)
    
    plt.title('è·¯ç½‘æ‹“æ‰‘')
    plt.xlabel('Xåæ ‡ (m)')
    plt.ylabel('Yåæ ‡ (m)')
    
    # é¢„æµ‹ç»“æœç¤ºä¾‹
    plt.subplot(1, 3, 2)
    sample_idx = 0
    node_idx = 0
    time_steps = range(graph_data['prediction_steps'])
    
    pred_speed = results['predictions'][sample_idx, :, node_idx, 0]
    target_speed = results['targets'][sample_idx, :, node_idx, 0]
    
    plt.plot(time_steps, target_speed, 'b-', label='å®é™…é€Ÿåº¦', linewidth=2)
    plt.plot(time_steps, pred_speed, 'r--', label='é¢„æµ‹é€Ÿåº¦', linewidth=2)
    plt.title(f'é€Ÿåº¦é¢„æµ‹ç¤ºä¾‹ (èŠ‚ç‚¹ {node_idx})')
    plt.xlabel('é¢„æµ‹æ­¥æ•°')
    plt.ylabel('é€Ÿåº¦ (km/h)')
    plt.legend()
    
    # æ€§èƒ½æŒ‡æ ‡
    plt.subplot(1, 3, 3)
    metrics = results['metrics']
    metric_names = list(metrics.keys())
    metric_values = list(metrics.values())
    
    bars = plt.bar(metric_names, metric_values)
    plt.title('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡')
    plt.ylabel('å€¼')
    plt.xticks(rotation=45)
    
    # åœ¨æ¡å½¢å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, value in zip(bars, metric_values):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.4f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('traffic_prediction_example.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return model, results, graph_data


def model_comparison_example():
    """æ¨¡å‹é…ç½®å¯¹æ¯”ç¤ºä¾‹"""
    print("\n=== æ¨¡å‹é…ç½®å¯¹æ¯”ç¤ºä¾‹ ===\n")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    data, coordinates = create_sample_data(n_timesteps=300, n_nodes=20, n_features=1)
    
    # æ•°æ®é¢„å¤„ç†
    processor = GraphDataProcessor(normalization='zscore')
    graph_data = processor.prepare_graph_data(data, coordinates, window_size=10, prediction_steps=2)
    
    # æ•°æ®é›†åˆ’åˆ†
    X = graph_data['X']
    y = graph_data['y']
    
    n_samples = X.shape[0]
    train_size = int(0.7 * n_samples)
    
    X_train = torch.FloatTensor(X[:train_size])
    y_train = torch.FloatTensor(y[:train_size])
    
    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
    
    graph_data_tensor = {
        'adj_matrix': torch.FloatTensor(graph_data['adj_matrix']),
        'laplacian': torch.FloatTensor(graph_data['laplacian']),
        'coordinates': torch.FloatTensor(graph_data['coordinates'])
    }
    
    # é…ç½®å¯¹æ¯”
    configs = [
        {
            'name': 'GCN + LSTM',
            'conv_type': 'gcn',
            'use_attention': False,
            'use_dynamic_adj': False,
            'n_hidden': 32,
            'n_layers': 2
        },
        {
            'name': 'ChebNet + Attention',
            'conv_type': 'cheb',
            'use_attention': True,
            'use_dynamic_adj': False,
            'n_hidden': 32,
            'n_layers': 2
        },
        {
            'name': 'GraphSAGE + Dynamic',
            'conv_type': 'sage',
            'use_attention': False,
            'use_dynamic_adj': True,
            'n_hidden': 32,
            'n_layers': 2
        },
        {
            'name': 'GAT + Full Features',
            'conv_type': 'gat',
            'use_attention': True,
            'use_dynamic_adj': True,
            'n_hidden': 32,
            'n_layers': 2
        }
    ]
    
    results = []
    
    for config in configs:
        print(f"è®­ç»ƒé…ç½®: {config['name']}")
        
        # åˆ›å»ºæ¨¡å‹
        model = GCNNetwork(
            n_nodes=graph_data['n_nodes'],
            n_features=graph_data['n_features'],
            n_hidden=config['n_hidden'],
            n_layers=config['n_layers'],
            conv_type=config['conv_type'],
            prediction_steps=graph_data['prediction_steps'],
            use_attention=config['use_attention'],
            use_dynamic_adj=config['use_dynamic_adj'],
            dropout=0.1
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = GCNTrainer(model, learning_rate=0.001)
        
        # è®­ç»ƒ
        train_losses = []
        for epoch in range(20):
            train_loss = trainer.train_epoch(train_loader, graph_data_tensor)
            train_losses.append(train_loss)
        
        # è®°å½•ç»“æœ
        results.append({
            'config': config,
            'final_loss': train_losses[-1],
            'train_losses': train_losses,
            'param_count': sum(p.numel() for p in model.parameters())
        })
        
        print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
        print(f"  å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        print()
    
    # å¯è§†åŒ–å¯¹æ¯”ç»“æœ
    plt.figure(figsize=(12, 8))
    
    # è®­ç»ƒæŸå¤±æ›²çº¿
    plt.subplot(2, 2, 1)
    for result in results:
        plt.plot(result['train_losses'], label=result['config']['name'])
    plt.title('è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”')
    plt.xlabel('Epoch')
    plt.ylabel('è®­ç»ƒæŸå¤±')
    plt.legend()
    plt.yscale('log')
    
    # æœ€ç»ˆæŸå¤±å¯¹æ¯”
    plt.subplot(2, 2, 2)
    config_names = [r['config']['name'] for r in results]
    final_losses = [r['final_loss'] for r in results]
    bars = plt.bar(config_names, final_losses)
    plt.title('æœ€ç»ˆè®­ç»ƒæŸå¤±å¯¹æ¯”')
    plt.ylabel('è®­ç»ƒæŸå¤±')
    plt.xticks(rotation=45)
    
    for bar, value in zip(bars, final_losses):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.4f}', ha='center', va='bottom')
    
    # å‚æ•°æ•°é‡å¯¹æ¯”
    plt.subplot(2, 2, 3)
    param_counts = [r['param_count'] for r in results]
    bars = plt.bar(config_names, param_counts)
    plt.title('æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯”')
    plt.ylabel('å‚æ•°æ•°é‡')
    plt.xticks(rotation=45)
    
    for bar, value in zip(bars, param_counts):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{value}', ha='center', va='bottom')
    
    # æ€§èƒ½æ€»ç»“è¡¨
    plt.subplot(2, 2, 4)
    plt.axis('off')
    
    # åˆ›å»ºæ€»ç»“è¡¨æ ¼
    table_data = []
    for result in results:
        config = result['config']
        table_data.append([
            config['name'],
            f"{result['final_loss']:.4f}",
            f"{result['param_count']:,}",
            config['conv_type'].upper(),
            "âœ“" if config['use_attention'] else "âœ—",
            "âœ“" if config['use_dynamic_adj'] else "âœ—"
        ])
    
    headers = ['é…ç½®', 'æœ€ç»ˆæŸå¤±', 'å‚æ•°æ•°é‡', 'å·ç§¯ç±»å‹', 'æ³¨æ„åŠ›', 'åŠ¨æ€é‚»æ¥']
    
    table = plt.table(cellText=table_data,
                     colLabels=headers,
                     cellLoc='center',
                     loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 1.5)
    plt.title('é…ç½®å¯¹æ¯”æ€»ç»“')
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # æ‰“å°æ€»ç»“
    print("=== é…ç½®å¯¹æ¯”æ€»ç»“ ===")
    print(f"{'é…ç½®':<20} {'æœ€ç»ˆæŸå¤±':<12} {'å‚æ•°æ•°é‡':<10} {'å·ç§¯ç±»å‹':<12} {'æ³¨æ„åŠ›':<8} {'åŠ¨æ€é‚»æ¥':<8}")
    print("-" * 80)
    for result in results:
        config = result['config']
        print(f"{config['name']:<20} {result['final_loss']:<12.6f} {result['param_count']:<10,} "
              f"{config['conv_type'].upper():<12} {'âœ“' if config['use_attention'] else 'âœ—':<8} "
              f"{'âœ“' if config['use_dynamic_adj'] else 'âœ—':<8}")
    
    return results


def save_and_load_example():
    """æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç¤ºä¾‹"""
    print("\n=== æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç¤ºä¾‹ ===\n")
    
    # 1. è®­ç»ƒä¸€ä¸ªæ¨¡å‹
    print("1. è®­ç»ƒåŸºç¡€æ¨¡å‹...")
    data, coordinates = create_sample_data(n_timesteps=200, n_nodes=15, n_features=1)
    
    processor = GraphDataProcessor(normalization='zscore')
    graph_data = processor.prepare_graph_data(data, coordinates, window_size=8, prediction_steps=1)
    
    # å¿«é€Ÿè®­ç»ƒ
    X = torch.FloatTensor(graph_data['X'][:50])
    y = torch.FloatTensor(graph_data['y'][:50])
    
    train_dataset = torch.utils.data.TensorDataset(X, y)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
    
    model = GCNNetwork(
        n_nodes=graph_data['n_nodes'],
        n_features=graph_data['n_features'],
        n_hidden=32,
        n_layers=2,
        conv_type='cheb',
        prediction_steps=graph_data['prediction_steps'],
        use_attention=True,
        use_dynamic_adj=False,
        dropout=0.1
    )
    
    trainer = GCNTrainer(model, learning_rate=0.001)
    
    graph_data_tensor = {
        'adj_matrix': torch.FloatTensor(graph_data['adj_matrix']),
        'laplacian': torch.FloatTensor(graph_data['laplacian']),
        'coordinates': torch.FloatTensor(graph_data['coordinates'])
    }
    
    # è®­ç»ƒå‡ ä¸ªepoch
    for epoch in range(10):
        trainer.train_epoch(train_loader, graph_data_tensor)
    
    print("   æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    # 2. ä¿å­˜æ¨¡å‹
    print("\n2. ä¿å­˜æ¨¡å‹...")
    
    # ä¿å­˜å®Œæ•´æ¨¡å‹ä¿¡æ¯
    model_info = {
        'model_state_dict': model.state_dict(),
        'model_config': {
            'n_nodes': graph_data['n_nodes'],
            'n_features': graph_data['n_features'],
            'n_hidden': 32,
            'n_layers': 2,
            'conv_type': 'cheb',
            'prediction_steps': graph_data['prediction_steps'],
            'use_attention': True,
            'use_dynamic_adj': False,
            'dropout': 0.1
        },
        'preprocessing_params': {
            'normalization': 'zscore',
            'adj_threshold': 0.1,
            'sigma2': 1.0,
            'epsilon': 0.1
        },
        'training_info': {
            'epochs_trained': 10,
            'learning_rate': 0.001,
            'optimizer': 'Adam'
        }
    }
    
    # ä¿å­˜ä¸ºä¸åŒæ ¼å¼
    torch.save(model_info, 'gcn_model_complete.pth')
    
    # åªä¿å­˜æ¨¡å‹å‚æ•°
    torch.save(model.state_dict(), 'gcn_model_weights.pth')
    
    # ä¿å­˜æ¨¡å‹é…ç½®
    config_path = 'gcn_model_config.json'
    with open(config_path, 'w') as f:
        json.dump(model_info['model_config'], f, indent=2)
    
    print(f"   å®Œæ•´æ¨¡å‹ä¿¡æ¯ä¿å­˜åˆ°: gcn_model_complete.pth")
    print(f"   æ¨¡å‹æƒé‡ä¿å­˜åˆ°: gcn_model_weights.pth")
    print(f"   æ¨¡å‹é…ç½®ä¿å­˜åˆ°: {config_path}")
    
    # 3. åŠ è½½æ¨¡å‹
    print("\n3. åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½å®Œæ•´æ¨¡å‹ä¿¡æ¯
    loaded_info = torch.load('gcn_model_complete.pth')
    
    # åˆ›å»ºæ–°æ¨¡å‹
    new_model = GCNNetwork(**loaded_info['model_config'])
    new_model.load_state_dict(loaded_info['model_state_dict'])
    
    print("   å®Œæ•´æ¨¡å‹ä¿¡æ¯åŠ è½½æˆåŠŸ")
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦åŠ è½½æ­£ç¡®
    new_model.eval()
    with torch.no_grad():
        test_input = X[:2]
        original_output = model(test_input, graph_data_tensor)
        loaded_output = new_model(test_input, graph_data_tensor)
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸€è‡´
        diff = torch.abs(original_output['predictions'] - loaded_output['predictions']).max()
        print(f"   æ¨¡å‹éªŒè¯: æœ€å¤§è¾“å‡ºå·®å¼‚ = {diff:.8f}")
        
        if diff < 1e-6:
            print("   âœ“ æ¨¡å‹åŠ è½½éªŒè¯é€šè¿‡")
        else:
            print("   âœ— æ¨¡å‹åŠ è½½éªŒè¯å¤±è´¥")
    
    # 4. æ¨¡å‹æ¨ç†ç¤ºä¾‹
    print("\n4. æ¨¡å‹æ¨ç†ç¤ºä¾‹...")
    
    # å‡†å¤‡æ–°çš„æµ‹è¯•æ•°æ®
    new_data, new_coordinates = create_sample_data(n_timesteps=50, n_nodes=15, n_features=1)
    new_graph_data = processor.prepare_graph_data(new_data, new_coordinates, window_size=8, prediction_steps=1)
    
    new_graph_data_tensor = {
        'adj_matrix': torch.FloatTensor(new_graph_data['adj_matrix']),
        'laplacian': torch.FloatTensor(new_graph_data['laplacian']),
        'coordinates': torch.FloatTensor(new_graph_data['coordinates'])
    }
    
    test_input = torch.FloatTensor(new_graph_data['X'][:5])
    
    # ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œæ¨ç†
    new_model.eval()
    with torch.no_grad():
        predictions = new_model(test_input, new_graph_data_tensor)
    
    print(f"   æ¨ç†è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"   é¢„æµ‹è¾“å‡ºå½¢çŠ¶: {predictions['predictions'].shape}")
    print(f"   é¢„æµ‹å€¼èŒƒå›´: [{predictions['predictions'].min():.4f}, {predictions['predictions'].max():.4f}]")
    
    # 5. æ¨¡å‹ä¿¡æ¯æ€»ç»“
    print("\n5. æ¨¡å‹ä¿¡æ¯æ€»ç»“...")
    print(f"   æ¨¡å‹ç±»å‹: GCN Network")
    print(f"   å·ç§¯ç±»å‹: {loaded_info['model_config']['conv_type']}")
    print(f"   éšè—ç»´åº¦: {loaded_info['model_config']['n_hidden']}")
    print(f"   å±‚æ•°: {loaded_info['model_config']['n_layers']}")
    print(f"   æ³¨æ„åŠ›: {'å¯ç”¨' if loaded_info['model_config']['use_attention'] else 'ç¦ç”¨'}")
    print(f"   åŠ¨æ€é‚»æ¥: {'å¯ç”¨' if loaded_info['model_config']['use_dynamic_adj'] else 'ç¦ç”¨'}")
    print(f"   è®­ç»ƒè½®æ•°: {loaded_info['training_info']['epochs_trained']}")
    print(f"   å­¦ä¹ ç‡: {loaded_info['training_info']['learning_rate']}")
    
    return new_model, predictions


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("=" * 60)
    print("GCNç½‘ç»œä½¿ç”¨ç¤ºä¾‹é›†åˆ")
    print("=" * 60)
    
    try:
        # 1. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
        basic_model, basic_results, basic_history = basic_usage_example()
        
        # 2. äº¤é€šæµé¢„æµ‹ç¤ºä¾‹
        traffic_model, traffic_results, traffic_graph_data = traffic_prediction_example()
        
        # 3. æ¨¡å‹é…ç½®å¯¹æ¯”ç¤ºä¾‹
        comparison_results = model_comparison_example()
        
        # 4. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç¤ºä¾‹
        loaded_model, inference_results = save_and_load_example()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("=" * 60)
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        print("- basic_example_results.png: åŸºç¡€ç¤ºä¾‹ç»“æœ")
        print("- traffic_prediction_example.png: äº¤é€šé¢„æµ‹ç¤ºä¾‹")
        print("- model_comparison.png: æ¨¡å‹å¯¹æ¯”ç»“æœ")
        print("- gcn_model_*.pth: ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶")
        print("- gcn_model_config.json: æ¨¡å‹é…ç½®æ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()